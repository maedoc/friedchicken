{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e0f3bd-274d-47de-ba12-0d385986ac51",
   "metadata": {},
   "source": [
    "The essential part of RWKV is the time and channel mixing, which we can code as modules.  This is adapted from\n",
    "\n",
    "https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py\n",
    "\n",
    "The model can work in two modes:\n",
    "\n",
    "- RNN behaves like a recurrent neural network and requires maintaining the state explicitly; below the code supports this mode for a single instance (not a batch).  Consequently each call does a single time step\n",
    "- Transformer (xfmr) mode works like a transformer, parallel over batches and channels with a full time loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc3731-486e-4aa7-bcc4-8a7636b24948",
   "metadata": {},
   "source": [
    "## util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f36bec-041f-47b3-b79a-fcb4d4a1d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def add_param(self, key, shape):\n",
    "        val = torch.ones(shape)/np.prod(shape)\n",
    "        setattr(self, key, torch.nn.Parameter(val))\n",
    "    def add_params(self, keys, shape):\n",
    "        for key in keys.split(' '):\n",
    "            self.add_param(key, shape)\n",
    "    def parameter_count(self):\n",
    "        return sum([p.numel() for p in self.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ded68-c887-4033-bde2-b4def9f5ff9c",
   "metadata": {},
   "source": [
    "## time mixing\n",
    "\n",
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1676e9-ccda-4275-865c-8c979f80a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeMix(MyModule):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.add_params('mix_k mix_v mix_r first decay', (C,))\n",
    "        self.layer_norm = torch.nn.LayerNorm(C)\n",
    "        \n",
    "        # https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/src/model.py#L174\n",
    "        self.time_shift = torch.nn.ZeroPad2d((0, 0, 1, -1))\n",
    "        self.key = torch.nn.Linear(C, C, bias=False)\n",
    "        self.value = torch.nn.Linear(C, C, bias=False)\n",
    "        self.receptance = torch.nn.Linear(C, C, bias=False)\n",
    "        self.output = torch.nn.Linear(C, C, bias=False)\n",
    "   \n",
    "    def forward(self, x, state=None):\n",
    "        # in train code, x.size is (batch, time, channel)\n",
    "        # so last_x is computed with time shift\n",
    "        \n",
    "        if x.ndim == 1:\n",
    "            # rnn style gen\n",
    "            B, T, C = 0, 0, x.shape[0]\n",
    "            rnn = True\n",
    "            _, last_x, aa, bb, pp = state\n",
    "        else:\n",
    "            B, T, C = x.shape\n",
    "            rnn = False\n",
    "            last_x = self.time_shift(x)\n",
    "            # https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/cuda/wkv_cuda.cu#L96\n",
    "            aa = torch.zeros_like(x[:,0]) # (B, C)\n",
    "            bb = torch.zeros_like(x[:,0])\n",
    "            pp = torch.ones_like(x[:,0])*(-1e38)\n",
    "    \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        xk = x * self.mix_k + last_x * (1 - self.mix_k)\n",
    "        xv = x * self.mix_v + last_x * (1 - self.mix_v)\n",
    "        xr = x * self.mix_r + last_x * (1 - self.mix_r)\n",
    "        \n",
    "        r = torch.sigmoid(self.receptance(xr))\n",
    "        k = self.key(xk)\n",
    "        v = self.value(xv)\n",
    "        \n",
    "        # rest is in the RUN_CUDA thingy\n",
    "        # RUN_CUDA(B, T, dim_att, time_decay, time_first, k, v)\n",
    "        # wkv_cuda.forward(B, T, C, w, u, k, v, y), y is output\n",
    "        # where dim_att==C is the attention dimension, n in this notebook\n",
    "        \n",
    "        # so where do aa bb pp come from?\n",
    "        # they are computed from start of sequence in cuda kernel\n",
    "        # https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/cuda/wkv_cuda.cu#L22\n",
    "        # not yet sure\n",
    "        \n",
    "        wkv = []\n",
    "        for t in range(1 if rnn else T): \n",
    "            # note confusing changes in variable names btw different codes\n",
    "            \n",
    "            # time_decay -> w, shape (C,)\n",
    "            # time_first -> u, shape (C,)\n",
    "            # k, v are same, shape (C,) or (B,T,C)\n",
    "            # y -> wkv, shape (C,) or (B,T,C)\n",
    "            \n",
    "            # k, v, y offset in cuda, k[block,:,c]\n",
    "            # thread grid computes wkv parallel for all B and C, iters T\n",
    "            kk = k if rnn else k[:,t]\n",
    "            vv = v if rnn else v[:,t]\n",
    "        \n",
    "            ww = self.first + kk        # u + kk;     (C,)+(B,C)->(B,C)\n",
    "            qq = torch.maximum(pp, ww)  # p = max(pp, ww); (B,C)->(B,C)\n",
    "            e1 = torch.exp(pp - qq)     # exp(pp - p)       %  \n",
    "            e2 = torch.exp(ww - qq)     # exp(ww - p)       %\n",
    "            \n",
    "            # y[ii], wkv[:,t,:]\n",
    "            wkv.append(\n",
    "                (e1 * aa + e2 * vv) / (e1 * bb + e2)   # (B,C)\n",
    "            )\n",
    "            \n",
    "            ww = pp + self.decay       # (B,C)+(C,)->(B,C)\n",
    "            qq = torch.maximum(ww, kk) # (B,C)\n",
    "            e1 = torch.exp(ww - qq)\n",
    "            e2 = torch.exp(kk - qq)\n",
    "            \n",
    "            # retain moving averages for next iter\n",
    "            aa = e1 * aa + e2 * vv\n",
    "            bb = e1 * bb + e2\n",
    "            pp = qq\n",
    "    \n",
    "        wkv = wkv[0] if rnn else torch.stack(wkv, dim=1)\n",
    "        out = self.output(r * wkv) # whence rwkv\n",
    "        \n",
    "        if rnn:  # rnn style return\n",
    "            return out, torch.stack((x, aa, bb, pp))\n",
    "        else: # parallel\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc9f07-8744-47b4-8a02-d1f26656e865",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "\n",
    "With the time mix module defined, we can test evaluation in RNN mode,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e83be7-c6a2-47fc-bd59-3aae3893b59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]), torch.Size([4, 6]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 2, 3, 6\n",
    "\n",
    "tm = TimeMix(C).to('mps')\n",
    "x = torch.randn(C).to('mps')\n",
    "state = torch.randn(5, C).to('mps')\n",
    "\n",
    "out, time_state = tm.forward(x, state)\n",
    "out.shape, time_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de6f83-78e6-4046-9462-89cda7cc1f31",
   "metadata": {},
   "source": [
    "and xfmr mode,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a193d8a9-47ef-49fe-821a-b2c1c74d129f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm = TimeMix(C).to('mps')\n",
    "x = torch.randn(B, T, C).to('mps')\n",
    "state = torch.randn(5, B, T, C).to('mps')\n",
    "tm.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148086f-fea1-4664-bad3-5e6fe7e60e01",
   "metadata": {},
   "source": [
    "## space mixing\n",
    "\n",
    "now space (\"channel\") mixing\n",
    "\n",
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d475dc0f-18a1-47d8-bcb8-0d5509469e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceMix(MyModule):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.add_params('mix_k mix_r', (C,))\n",
    "        # self.add_params('kw vw rw', (C, C))\n",
    "        self.layer_norm = torch.nn.LayerNorm(C)\n",
    "        # https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/src/model.py#L247\n",
    "        # uses a time shift for last_x and nn.Linear(..., bias=False)\n",
    "        self.time_shift = torch.nn.ZeroPad2d((0, 0, 1, -1))\n",
    "        # instead of weights\n",
    "        # also need to copy initialization tricks\n",
    "        self.kw = torch.nn.Linear(C, C, bias=False)\n",
    "        self.vw = torch.nn.Linear(C, C, bias=False)\n",
    "        self.rw = torch.nn.Linear(C, C, bias=False)\n",
    "        \n",
    "    def forward(self, x, state=None):\n",
    "        \n",
    "        if x.ndim == 1: # rnn mode, x.shape is (C)\n",
    "            last_x, *_ = state\n",
    "        else: # xfmr, x.shape is (B,T,C)\n",
    "            last_x = self.time_shift(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        xk = x * self.mix_k + last_x * (1 - self.mix_k)\n",
    "        xr = x * self.mix_r + last_x * (1 - self.mix_r)\n",
    "        r = torch.sigmoid(self.rw(xr))\n",
    "        k = torch.square(torch.relu(self.kw(xk))) # square relu, primer paper\n",
    "        rvwk = r * self.vw(k)\n",
    "        \n",
    "        if x.ndim == 1: # rnn\n",
    "            return rvwk, x\n",
    "        else: # xmfr\n",
    "            return rvwk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7ba09-583b-4fa4-926f-125c07e31b3b",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e727f11e-ecb3-4f1c-854b-d8a55e254076",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SpaceMix(C).to('mps')\n",
    "x = torch.randn(B, T, C).to('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f286ac2-c842-4b98-87d2-fb0dacec77ef",
   "metadata": {},
   "source": [
    "xfmr mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b122ac93-4056-422d-8fdf-e256f0bec2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8aa94e-f375-4f3b-80be-7890f0136aae",
   "metadata": {
    "tags": []
   },
   "source": [
    "rnn mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67348fb0-9ff4-49de-9147-2b8fa3cded8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]), torch.Size([6]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.randn(5, C).to('mps')\n",
    "rvw, x = sm.forward(x[0,0], state)\n",
    "rvw.shape, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c62e90-69df-4157-bb15-5fabfe2e17c1",
   "metadata": {},
   "source": [
    "## block structure\n",
    "\n",
    "a RWKV block groups the time and space mixing together,\n",
    "\n",
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53dc7bf1-1ac1-406b-bed9-a6c452e458d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(MyModule):\n",
    "\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.time_mix = TimeMix(C)\n",
    "        self.space_mix = SpaceMix(C)\n",
    "        self.time_ln = torch.nn.LayerNorm(C)\n",
    "        self.space_ln = torch.nn.LayerNorm(C)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        if x.ndim == 1: # rnn mode\n",
    "            # in tvb terms, as if we're starting with afferent states\n",
    "            x_ = self.time_ln(x)\n",
    "            time_dx, time_state = self.time_mix(x_, state)\n",
    "            x = x + time_dx\n",
    "            x_ = self.space_ln(x)\n",
    "            space_dx, space_state = self.space_mix(x_, state)\n",
    "            x = x + space_dx\n",
    "            next_state = torch.concatenate((space_state.reshape((1, -1)), time_state))\n",
    "            return x, next_state\n",
    "        else:\n",
    "            x = x + self.time_mix(self.time_ln(x))\n",
    "            x = x + self.space_mix(self.space_ln(x))\n",
    "            return x\n",
    "\n",
    "block = Block(C).to('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5095e-f212-4804-ba5f-be13f46f9709",
   "metadata": {},
   "source": [
    "### eval\n",
    "\n",
    "rnn mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57385c1e-9bf5-4f73-8f3c-154c72bc26f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]), torch.Size([5, 6]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(C).to('mps')\n",
    "state = torch.randn(5, C).to('mps')\n",
    "\n",
    "nx, nstate = block.forward(x, state)\n",
    "nx.shape, nstate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b303fd-47a0-40fb-803d-a401ee72430e",
   "metadata": {},
   "source": [
    "xfmr mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7534f3e9-0d96-4d99-a323-0e6da5225574",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B, T, C).to('mps')\n",
    "nx = block.forward(x)\n",
    "nx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806fe30-6d50-44e6-ba96-2550d82f8944",
   "metadata": {},
   "source": [
    "pretty sure xfmr is linear runtime w/ T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03a91995-f2d1-4758-a2d5-bde240aaebcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856 µs ± 3.12 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "5.07 ms ± 53.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "44.6 ms ± 325 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "block = Block(C)\n",
    "\n",
    "for T_ in [10, 100, 1000]:\n",
    "    x = torch.randn(B, T_, C)\n",
    "    %timeit block.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5eb1b-0a06-4f0e-a54e-e25af07df4f7",
   "metadata": {},
   "source": [
    "## multiblock\n",
    "### module\n",
    "the actual RWKV uses several blocks, with layer norm in and out and a linear decode,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ca02b2d-eaac-48ff-9cc4-c0115716d96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RWKV(MyModule):\n",
    "\n",
    "    def __init__(self, C, nlayers=1, indim=None, outdim=None):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.indim = indim or C\n",
    "        self.outdim = outdim or C\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            Block(C) for i in range(nlayers)])\n",
    "        self.pre_ln = torch.nn.LayerNorm(C)\n",
    "        self.post_ln = torch.nn.LayerNorm(C)\n",
    "        self.encode = torch.nn.Linear(self.indim, C, bias=False)\n",
    "        self.decode = torch.nn.Linear(C, self.outdim, bias=False)\n",
    "    \n",
    "    def forward(self, x, state=None):\n",
    "        x = self.encode(x)\n",
    "        x = self.pre_ln(x)\n",
    "        for layer in self.layers:\n",
    "            if x.ndim == 1:\n",
    "                x, state = layer(x, state)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = self.post_ln(x)\n",
    "        x = self.decode(x)\n",
    "        # here I'm skipping the softmax since we don't want to force probabilities\n",
    "        if x.ndim == 1:\n",
    "            return x, state\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3c452-9d8a-497d-8ea5-1ca0a2b23466",
   "metadata": {},
   "source": [
    "### eval\n",
    "\n",
    "since we want a little flexibility with dimensions, need a few more test cases here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29ee2738-2841-4e18-8d77-ccd4f7ec725d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwkv = RWKV(C, 3)\n",
    "rwkv.parameter_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dffd31-e51b-4ec6-b488-161bc65822d8",
   "metadata": {},
   "source": [
    "#### rnn mode\n",
    "\n",
    "basic usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2e4cd59-ae38-4312-aafb-f51bdf872062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(C)\n",
    "state = torch.randn(5, C)\n",
    "rwkv = RWKV(C, 3)\n",
    "nx, nstate = rwkv(x, state)\n",
    "assert nx.shape == x.shape and nstate.shape == state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab80fd-9beb-4705-a230-fd2a8e7e1d46",
   "metadata": {},
   "source": [
    "extra input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49550b76-e7ea-4635-9451-3f6e9712a26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(C + 3)\n",
    "state = torch.randn(5, C)\n",
    "rwkv = RWKV(C, 3, indim=C + 3)\n",
    "nx, nstate = rwkv(x, state)\n",
    "assert nx.shape[0] == C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87a0c0-1376-41b3-8359-7b85d83b1491",
   "metadata": {},
   "source": [
    "extra carried through the network but not in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90373704-9e83-451d-9671-2208bee9a73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(C + 3)\n",
    "state = torch.randn(5, C + 3)\n",
    "rwkv = RWKV(C + 3, 3, outdim=C)\n",
    "nx, nstate = rwkv(x, state)\n",
    "assert nx.shape[0] == C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0fb68-ac40-4da5-9fa2-a56238d0bfb6",
   "metadata": {},
   "source": [
    "extra inputs, large latent state, smaller output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40fef45c-1423-4853-94e5-7f8c17d9e4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4980"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indim = C + 3\n",
    "latdim = 2*C\n",
    "outdim = C\n",
    "\n",
    "x = torch.randn(indim)\n",
    "state = torch.randn(5, latdim)\n",
    "rwkv = RWKV(latdim, 4, indim=indim, outdim=outdim)\n",
    "nx, nstate = rwkv(x, state)\n",
    "\n",
    "assert nx.shape==(outdim,) and nstate.shape==(5, latdim)\n",
    "\n",
    "rwkv.parameter_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb2ec4-c928-4123-a586-8257354bb7da",
   "metadata": {},
   "source": [
    "#### xfmr mode\n",
    "\n",
    "same idea but in xfmr mode: check different usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d07777be-c009-4665-840b-f92b70871213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(B, T, C)\n",
    "rwkv = RWKV(C, 3)\n",
    "nx = rwkv(x)\n",
    "assert x.shape == nx.shape\n",
    "\n",
    "x = torch.randn(B, T, C + 3)\n",
    "rwkv = RWKV(C, 3, indim=C + 3)\n",
    "nx = rwkv(x)\n",
    "assert x.shape[2]-3 == nx.shape[2]\n",
    "\n",
    "x = torch.randn(B, T, C + 3)\n",
    "rwkv = RWKV(C + 3, 3, outdim=C)\n",
    "nx = rwkv(x)\n",
    "assert x.shape[2]-3 == nx.shape[2]\n",
    "\n",
    "indim = C + 3\n",
    "latdim = 2*C\n",
    "outdim = C\n",
    "x = torch.randn(B, T, indim)\n",
    "rwkv = RWKV(latdim, 4, indim=indim, outdim=outdim)\n",
    "nx = rwkv(x)\n",
    "assert nx.shape==(B, T, outdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc5953-42dc-4856-be40-e49cd645b9ad",
   "metadata": {},
   "source": [
    "## notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621853a-9fe6-40e1-8787-4f1c9ac86315",
   "metadata": {},
   "source": [
    "To note, this seems to be the non-parallel RNN version from the 150 line impl in ChatRWKV which explicit passes state.  That doesn't happen in the training code, so not yet sure what's up with that.\n",
    "\n",
    "Lastly the full rwkv wraps several blocks with layer norms so it's always probability over discrete distribution.  For time series model, it would be either the next time point or normal dist over next time point (for SDE interpretation).\n",
    "\n",
    "interpreting the output as a probabilistic derivative is interesting but presents a sampling problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e8500-9ac6-43b7-87e0-18f7a9676ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
